begin
sys.rqScriptDrop('CLASSRFCHURN');
sys.rqScriptCreate('CLASSRFCHURN',
                 'function(p_dataframe,perc_over,perc_under,kn,isR,p_spltratio,prim_class_label,pY,p_imputetype){                 
                      #--------------------------------------------------------------------------
                      ## FORMULA for Model Building
                      #--------------------------------------------------------------------------
                      
                      p_formula <- as.formula(paste(tail(names(p_dataframe), 1) , paste(head(names(p_dataframe), -1), collapse=" + "), sep=" ~ "))
                      
                      #--------------------------------------------------------------------------
                      ## PARAMETERS for Model Building
                      #--------------------------------------------------------------------------                        
                      p_topn = 20                    
                      #-------------------------------------------------------------------------
                      ## 0.Data splitting into train and validation Set
                      #-------------------------------------------------------------------------
                      data.split <- function(df,resVal,seedvalue,spltratio)
                      {
                        library(caTools)
                        set.seed(100)
                        split <- sample.split(Y = df[ ,resVal],SplitRatio = spltratio)				  
                      }            			
                      
                      #-------------------------------------------------------------------------
                      ## 1.Building Random Forest Model
                      #-------------------------------------------------------------------------
                    
                      mdl.training.RF <- function(p_trainx,p_trainy){
                        library(randomForest)
                        
                        get_oob_err <- function(model){
                          a <- capture.output(model)[8]
                          m <- gregexpr("[0-9]+.[0-9]+",a)
                          return(as.numeric(regmatches(a,m)[[1]]))
                        }
                        
                        # Intialize err_arr
                        err_arr <- c()
                        ntree_vec <- c(101,201,301,401,501,601,701,801,901,1001,1101,1201,1301,1401,1501)
                        
                        # Looping through various ntree values
                        for(i in 1:length(ntree_vec))
                        {
                          #set.seed(456)
                          model <- randomForest(x = p_trainx,y = as.factor(p_trainy),ntree=ntree_vec[i])
                          err_arr[i] <- get_oob_err(model)
                        }
                        print(err_arr)
                        
                        # Optimum No. of Trees
                        opti_ntrees <- min(ntree_vec[which(err_arr == min(err_arr))])
                        print(opti_ntrees)
                        
                        # mtry tuning
                        trf <- tuneRF(x = p_trainx ,y = as.factor(p_trainy), mtryStart=2, ntreeTry=1501, stepFactor=2, improve=0.05, trace=T, plot=FALSE)
                        opti_mtry <- trf[which.min(trf[ ,"OOBError"])]
                        
                        # Building Model WITH tuned mtry and ntrees
                        #set.seed(456)
                        mdl <- randomForest(x = p_trainx,y = p_trainy,ntree=opti_ntrees,mtry=opti_mtry,importance = TRUE,keep.forest = T)  
                        
                        return(mdl)
                      }
                      
                      #-------------------------------------------------------------------------
                      ## 3.Model Metrics Display
                      # What percent of your predictions were correct?- Accuracy                    
                      # What percent of the positive cases did you catch? - recall
                      # What percent of positive predictions were correct? - precision
                      #-------------------------------------------------------------------------
                      metricROCR <- function(model,testdata,pY,ds.name,p_class_label)
                      {
                        library(ROCR)
                        library(ggplot2)
                        
                        # Predict
                        predDT <- predict(object = model,newdata = testdata,type = "class")
                        predDTprob <- predict(object = model,newdata = testdata,type = "prob")
                        
                        # Creating predicted probability and actual label predicted dataframe
                        pred.df <- data.frame(predicted=as.double(predDTprob[,1]),actual=as.numeric(ifelse(testdata[,pY]==p_class_label,1,0)))
                        pred.df <- pred.df[order(pred.df$predicted, decreasing=TRUE), ]
                        
                        # Calculate ROCR Prediction
                        pred.rocr <- prediction(predictions = pred.df$predicted, labels = pred.df$actual)
                    
                        # Stats
                        roc.perf <- performance(pred.rocr, measure = "tpr", x.measure = "fpr")
                        tpr.perf <- performance(pred.rocr, measure = "tpr")
                        fpr.perf <- performance(pred.rocr, measure = "fpr")
                        fnr.perf <- performance(pred.rocr, measure = "fnr")
                        tnr.perf <- performance(pred.rocr, measure = "tnr")
                        recall.perf <- performance(pred.rocr, measure = "prec", x.measure = "rec")
                        sensspec.perf <- performance(pred.rocr, measure = "sens", x.measure = "spec")
                        lift.perf <- performance(pred.rocr, measure = "lift", x.measure = "rpp")
                        auc.perf <- performance(pred.rocr, measure = "auc")
                        accuracy.perf <- performance(pred.rocr, measure = "acc")
                        err.perf <- performance(pred.rocr, measure = "err")
                        calibration.perf <- performance(pred.rocr, measure = "cal")
                        pcmiss.perf <- performance(pred.rocr,"pcmiss","lift")
                        prbe.perf <- performance(pred.rocr, "prbe")
                        
                        ## Scores ##
                        
                        # AUC Score
                        auc_score <- auc.perf@y.values[[1]]
                        
                        # Precision/Recall breakeven Score
                        prbe.score <- prbe.perf@x.values[[1]]
                        
                        # Accuracy Rate Score
                        acc_rate <- max(accuracy.perf@y.values[[1]])
                        
                        # Accuracy Rate gt 50% Score
                        acc_roc_gt_50 <- accuracy.perf@y.values[[1]][max(accuracy.perf@x.values[[1]] > 0.5)]
                        
                        # Error Rate Score
                        error_rate <- min(err.perf@y.values[[1]])    
                        
                        # Draw ROC curve
                        plot(roc.perf, main="ROC with Convex Hull", colorize=TRUE,print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2)
                        ch = performance(pred.rocr , "rch")
                        plot(ch, add = TRUE, lty = 2)
                        
                        # Recall-Precision Plot
                        plot(recall.perf,colorize = T,print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2,main = "Recall-Precision Plot")
                        
                        # Sensitivity-Specificity Plot
                        plot(sensspec.perf,colorize = T,print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2,main = "Sensitivity vs Specificity")
                        
                        # Lift Plot
                        plot(lift.perf,colorize = T,print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2,main = "Lift Plot")
                        
                        # Accuracy - Boxplot (Spread)
                        plot(accuracy.perf, avg= "vertical", spread.estimate="boxplot", show.spread.at= seq(0.1, 1.0, by=0.1),main = "Accuracy - Boxplot (Spread)")
                        
                        # Accuracy vs Cutoff
                        # Get the cutoff for the best accuracy
                        bestAccInd <- which.max(accuracy.perf@"y.values"[[1]])
                        bestMsg <- paste("best accuracy=", accuracy.perf@"y.values"[[1]][bestAccInd],"at cutoff=", round(accuracy.perf@"x.values"[[1]][bestAccInd], 4))
                        plot(accuracy.perf, sub=bestMsg,main = "Accuracy vs Cutoff")
                        
                        # TPR vs Cutoff
                        plot(tpr.perf,main = "TPR vs Cutoff")
                        
                        # TNR vs Cutoff
                        plot(tnr.perf,main = "TNR vs Cutoff")
                        
                        # FPR vs Cutoff
                        plot(fpr.perf,main = "FPR vs Cutoff")
                        
                        # FNR vs Cutoff
                        plot(fnr.perf,main = "FNR vs Cutoff")
                        
                        # Prediction-conditioned miss 
                        plot(pcmiss.perf, colorize=T, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(1.2,1.2), avg="threshold", lwd=3)
                        
                        ## Various Metric Plots ##
                        # pos/neg densities
                        print(ggplot(data=pred.df,aes(x=predicted)) + geom_density(aes(fill=factor(actual)), size=1, alpha=.3) +
                                scale_x_continuous("Predicted", breaks=(0:4)/4, limits=c(0,1), labels=sprintf("%d%%", (0:4)*25)) +
                                scale_y_sqrt("Density") + scale_fill_manual(values = c("red","green")) + ggtitle(label = "Label Separation Density Curve"))
                        
                        # Confusion Matrix
                        confusionMatrix <- table(predDT ,testdata[,pY])
                        confusionMatrix
                        
                        ore.save(confusionMatrix,name = ds.name,append = TRUE)
                        
                        # Basic measures calculation from CM
                        TP <- confusionMatrix[1,1]
                        TN <- confusionMatrix[2,2]
                        FN <- confusionMatrix[1,2]
                        FP <- confusionMatrix[2,1]
                        TOT <- sum(confusionMatrix)      
                        
                        # Measures
                        Accuracy <- round(as.numeric((TP + TN)/TOT),3)
                        ErrorRate <- 1 - Accuracy
                        FPR <- round(as.numeric(FP/(TN+FP)),3)
                        Recall <- round(as.numeric(TP/(TP+FN)),3)
                        Specificity <- round(as.numeric(TN/(TN + FP)),3)
                        Precision <- round(as.numeric(TP/(TP+FP)),3)
                        Fvalue <- round(as.numeric(2*Recall*Precision/(Recall + Precision)),3)
                        
                        df <- rbind(auc_score = auc_score,acc_rocr = acc_rate,accuracy = Accuracy,acc_rocr_gt50 = acc_roc_gt_50,error_rate_rocr = error_rate,error_rate = ErrorRate,fpr = FPR,recall = Recall,specificity = Specificity,precision = Precision,fval = Fvalue)
                        #colnames(df) <- "scores"
                        perf_metric <- data.frame(name=rownames(df),score=df,row.names = NULL)   
                        perf_metric
                      }                
                      
                      # -------------------------------------------------------------------------
                      ## 4.Model Decision Tree prediction
                      # -------------------------------------------------------------------------
                      mdl.pred.RF <- function(model,validationset,predtype="class")
                      {
                        # predicting on the validation Set
                        predtree <- predict(object = model,newdata = validationset,type = predtype)  
                        return(predtree)
                      }    
                      
                      # -------------------------------------------------------------------
                      ## 5.Data Imputation
                      #--------------------------------------------------------------------
                      
                      impute.data <- function(data,imputetype="mean")
                      {
                        ## This function imputes NA by mean or median values
                        if(imputetype == "mean"){
                          for (i in which(sapply(data, is.numeric))) {
                            data[is.na(data[, i]), i] <- mean(data[, i],  na.rm = TRUE)
                          }
                        } else if(imputetype == "median") {
                          for (i in which(sapply(data, is.numeric))) {
                            data[is.na(data[, i]), i] <- median(data[, i],  na.rm = TRUE)
                          }
                        } else if(imputetype == "knn"){
                          library(DMwR)      
                          data <- knnImputation(data = data)
                        }else{
                          stop("wrong imputation type.Only mean,median and knn is supported")
                        }
                        
                        return(data)
                      }	
                      
                      # -------------------------------------------------------------------
                      ## 6.Converting formula to vectors of string
                      #--------------------------------------------------------------------	
                      
                      x_variables <- function(form,Y)
                      {
                        aa <- gsub(pattern =" ",replacement="",x=paste0(format(form), collapse = ""))
                        bb <- gsub(pattern = "[+~]",replacement=",",x=aa)
                        cc <- unlist(strsplit(x = bb,split = "[,]"))
                        xcols <- cc[!cc %in% c(Y)]			  
                        return(xcols)
                      }
                      
                      # -------------------------------------------------------------------
                      ## 7.Rare Class Balancing
                      #--------------------------------------------------------------------	                      
                      imbalance_correction <- function(form,data,perc.over,perc.under,k,isRequired)
                      {
                        # Handling class Imbalance
                        library(DMwR)
                        
                        if(isRequired == "Y"){
                          data_bal <- SMOTE(form = form,data = data,perc.over = perc.over,k = k,perc.under = perc.under)
                          
                          
                          return(data_bal)
                        }
                        else{
                          return(data)
                        }
                      }
                      
                      ######################################################
                      ## THE MAIN FUNCTION ##
                      ## This is the Entry point
                      ######################################################                  	    
                      
                      # Pulling dataset into ORE transparency Layer
                      dataset <- ore.pull(p_dataframe)        
                      
                      ## Find which columns are factors
                      factor_cols <- names(dataset)[sapply(dataset,is.character)]            
                      
                      ## Converting training character columns to factor
                      dataset[, factor_cols] <- lapply(dataset[, factor_cols], as.factor) 
                      
                      # Releveling
                      #dataset[ ,pY] <- relevel(factor(dataset[ ,pY]),prim_class_label)       
                      dataset[ ,pY] <- relevel(dataset[ ,pY],prim_class_label)  
                      
                      ## Data splitting into training and validation set
                      splt <- data.split(df = dataset,resVal = pY,seedvalue = 1000,spltratio = p_spltratio)
                      
                      # Creation of training and validation set
                      training <- subset(x = dataset,splt == TRUE)
                      validation <- subset(x = dataset,splt == FALSE)
                      
                      # Handling class Imbalance
                      training_bal <- imbalance_correction(form = p_formula,data = training,perc.over = perc_over,perc.under = perc_under,k = kn,isRequired = isR)
                      #training_bal[,pY] <- as.factor(training_bal[,pY])
                      
                      unbal <- table(training[,pY])
                      bal <- table(training_bal[,pY])
                      
                      # Imputing the NA on main dataset
                      training_imputed <- impute.data(data = training_bal,imputetype = p_imputetype)
                      validation_imputed <- impute.data(data = validation,imputetype = p_imputetype)    
                      
                      print("#################################################")
                      print("***1.Model Building -  RandomForest...")
                      print("#################################################")
                      
                      # Extracting only X columns
                      p_trainx <- training_imputed[ ,x_variables(p_formula,pY)]
                      
                      # Extracting Y column
                      p_trainy <- training_imputed[,pY]
                      
                      ## Conversion of CHAR columns to FACTOR
                      # Training Data Predictors
                      p_trainx[sapply(p_trainx, is.character)] <- lapply(p_trainx[sapply(p_trainx, is.character)], as.factor)
                      
                      # Validation Data Predictors
                      validation_imputed[sapply(validation_imputed, is.character)] <- lapply(validation_imputed[sapply(validation_imputed, is.character)], as.factor)
                      
                      print("***2.Training the RF Model")
                      #modRF <- mdl.training.RF(p_trainx = p_trainx,p_trainy = p_trainy,p_ntree = pntree)
                      modRF <- mdl.training.RF(p_trainx = p_trainx,p_trainy = p_trainy)
                      
                      print("***3.Random Forest Variable Importance")
                      print(varImpPlot(modRF))
                      
                      varImp <- modRF$importance
                      
                      
                      print("#################################################")
                      print("***.COMPLETED - Model Building -  Random Forest...")
                      print("#################################################")
                      
                      #print("Saving the RF Model")
                      #save(modRF,file = paste0(p_storageDir,"/","trainedModRF.rda"))
                      
                      if (nrow(ore.datastore(name=ds.name)) > 0 ) 
                      {
                        ore.delete(name = ds.name)
                      }
                      ore.save(modRF,name = ds.name,append = TRUE)
                      ore.save(varImp,name = ds.name,append= TRUE)
                      ore.save(unbal,name = ds.name,append= TRUE)
                      ore.save(bal,name = ds.name,append= TRUE)
                      
                      print("***3.Model Metrics...")
                      
                      summary(validation_imputed)
                      mdl_metric_RF <- metricROCR(model = modRF,testdata = validation_imputed,pY = pY,ds.name = ds.name,p_class_label=prim_class_label)
                      
                      # Returning the Metrics                   
                      mdl_metric_RF
                 }');
end;

#####################################################################################################################
select * from table(rqTableEval(
                    cursor(select GENDER, AGE, TENURE, PERF_SCORE_Y2011, PERF_SCORE_Y2012, PERF_SCORE_Y2013, 
                    PERF_SCORE_Y2014, TOT_SICK_LEAVES_L4Y , NO_OF_PROMO_L4Y, NO_OF_SVR_CHNG_L4Y , 
                    NO_OF_SALREV_L4Y , NO_OF_DEPT_CHNG_L4Y , Y2011_CR , Y2012_CR , Y2013_CR , Y2014_CR, ISCHURN from AA_ETISALAT_TRAIN_DS2),
                    cursor(select 1 as "ore.connect",300 "perc_over",200 "perc_under",20 "kn",'Y' "isR",0.80 "p_spltratio",'Y' "prim_class_label",'ISCHURN' "pY" ,'mean' "p_imputetype",'dsRF_churn_store' "ds.name" from dual),
                    'select CAST(1 as VARCHAR2(20)) METRIC, 1 SCORE from dual',
                    'CLASSRFCHURN'));
                    
select * from table(rqTableEval(
                    cursor(select GENDER, AGE, TENURE, PERF_SCORE_Y2011, PERF_SCORE_Y2012, PERF_SCORE_Y2013, 
                    PERF_SCORE_Y2014, TOT_SICK_LEAVES_L4Y , NO_OF_PROMO_L4Y, NO_OF_SVR_CHNG_L4Y , 
                    NO_OF_SALREV_L4Y , NO_OF_DEPT_CHNG_L4Y , Y2011_CR , Y2012_CR , Y2013_CR , Y2014_CR, ISCHURN from AA_ETISALAT_TRAIN_DS2),                    
                    cursor(select 1 as "ore.connect",300 "perc_over",200 "perc_under",20 "kn",'Y' "isR",0.80 "p_spltratio",'Y' "prim_class_label",'ISCHURN' "pY" ,'mean' "p_imputetype",'dsRF_churn_store' "ds.name" from dual),
                    'PNG',
                    'CLASSRFCHURN'));
