
library(ORE)
ore.connect("rquser",service_name="orcl",host="localhost",password="welcome1", all=TRUE)

p_dataframe <- AA_EMP_ATTRITION_TRAIN_MASTER

#--------------------------------------------------------------------------
## FORMULA for Model Building
#--------------------------------------------------------------------------
p_formula <- formula(ISCHURN ~ GENDER + AGE + TENURE + PERF_SCORE_Y2011 + PERF_SCORE_Y2012 + PERF_SCORE_Y2013 + PERF_SCORE_Y2014 + 
                         AVG_PERF_SCORE_L4Y + TOT_SICK_LEAVES_L4Y + NO_OF_PROMO_L4Y + NO_OF_SVR_CHNG_L4Y + NO_OF_SALREV_L4Y + 
                         NO_OF_DEPT_CHNG_L4Y)                

#--------------------------------------------------------------------------
## PARAMETERS for Model Building
#--------------------------------------------------------------------------               
pY = "ISCHURN"
prim_class_label = "Y"
p_imputetype = "mean"
p_spltratio = 0.70
p_storageDir = "D:/R/rworkspace/tutorial/ssiout"
pntree = 501
p_topn = 20
ds.name = "dsRF_churn_store"

#----------------- Dataset Balancing -------------------------------------

perc_over <- 200    # Percentage of Oversampling
perc_under <- 150   # Percentage of Undersampling
kn <- 50            # How many Nearest Neighbours.
isR <- 'N'          # Whether data balancing is needed.

#-------------------------------------------------------------------------
## 0.Data splitting into train and validation Set
#-------------------------------------------------------------------------
data.split <- function(df,resVal,seedvalue,spltratio)
{
  library(caTools)
  set.seed(100)
  split <- sample.split(Y = df[ ,resVal],SplitRatio = spltratio)				  
}            			

#-------------------------------------------------------------------------
## 1.Building Random Forest Model
#-------------------------------------------------------------------------
mdl.training.RF <- function(p_trainx,p_trainy,p_ntree){
  # Finding optimum mtry
  library(randomForest)
  trf <- tuneRF(x = p_trainx ,y = as.factor(p_trainy), mtryStart=2, ntreeTry=1501, stepFactor=2, improve=0.05, trace=T, plot=FALSE)
  opti_mtry <- trf[which.min(trf[ ,"OOBError"])]
  
  # Building Model
  mdl <- randomForest(x = p_trainx,y = as.factor(p_trainy),ntree=p_ntree,mtry=opti_mtry)  
  #mdl <- randomForest(x = p_trainx,y = p_trainy,ntree=p_ntree,mtry=2)
  return(mdl)
} 

#-------------------------------------------------------------------------
## 2.RandomForest variable Importance
#-------------------------------------------------------------------------
mdl.rf.var.importance <- function(modelfit,title,barcolor="grey",topn){  
  library(ggplot2)
  rf_imp <- data.frame(variable = rownames(modelfit$importance),MeanDecreaseGini=modelfit$importance,row.names = NULL)
  rf_imp$variable <- factor(rf_imp$variable,levels = rf_imp[order(rf_imp$MeanDecreaseGini),"variable"])      
  ret_df <- data.frame(head(rf_imp[order(-rf_imp$MeanDecreaseGini),],topn),row.names = NULL)
  
  gg <- ggplot(data = ret_df,aes(x=variable,y=MeanDecreaseGini)) 
  varImpchart <- gg + geom_bar(stat="identity",fill=barcolor) + coord_flip() + ggtitle(title)
  print(varImpchart)
  return(ret_df)
}

#-------------------------------------------------------------------------
## 3.Model Metrics Display
#-------------------------------------------------------------------------
metricROCR <- function(model,testdata,pY,ds.name,p_class_label)
{
  library(ROCR)
  library(ggplot2)
  
  # Predict
  predDT <- predict(object = model,newdata = testdata,type = "class")
  predDTprob <- predict(object = model,newdata = testdata,type = "prob")
  
  # Creating predicted probability and actual label predicted dataframe
  pred.df <- data.frame(predicted=as.double(predDTprob[,1]),actual=as.numeric(ifelse(testdata[,pY]==p_class_label,1,0)))
  pred.df <- pred.df[order(pred.df$predicted, decreasing=TRUE), ]
  
  # Calculate ROCR Prediction
  pred.rocr <- prediction(pred.df$predicted, pred.df$actual)
  
  # Stats
  roc.perf <- performance(pred.rocr, measure = "tpr", x.measure = "fpr")
  tpr.perf <- performance(pred.rocr, measure = "tpr")
  fpr.perf <- performance(pred.rocr, measure = "fpr")
  fnr.perf <- performance(pred.rocr, measure = "fnr")
  tnr.perf <- performance(pred.rocr, measure = "tnr")
  recall.perf <- performance(pred.rocr, measure = "prec", x.measure = "rec")
  sensspec.perf <- performance(pred.rocr, measure = "sens", x.measure = "spec")
  lift.perf <- performance(pred.rocr, measure = "lift", x.measure = "rpp")
  auc.perf <- performance(pred.rocr, measure = "auc")
  accuracy.perf <- performance(pred.rocr, measure = "acc")
  err.perf <- performance(pred.rocr, measure = "err")
  calibration.perf <- performance(pred.rocr, measure = "cal")
  pcmiss.perf <- performance(pred.rocr,"pcmiss","lift")
  prbe.perf <- performance(pred.rocr, "prbe")
  
  ## Scores ##
  
  # AUC Score
  auc_score <- auc.perf@y.values[[1]]
  
  # Precision/Recall breakeven Score
  prbe.score <- prbe.perf@x.values[[1]]
  
  # Accuracy Rate Score
  acc_rate <- max(accuracy.perf@y.values[[1]])
  
  # Accuracy Rate gt 50% Score
  acc_roc_gt_50 <- accuracy.perf@y.values[[1]][max(accuracy.perf@x.values[[1]] > 0.5)]
  
  # Error Rate Score
  error_rate <- min(err.perf@y.values[[1]])
  
  ## Various Metric Plots ##
  # pos/neg densities
  ggplot(data=pred.df,aes(x=predicted)) + geom_density(aes(fill=factor(actual)), size=1, alpha=.3) +
    scale_x_continuous("Predicted", breaks=(0:4)/4, limits=c(0,1), labels=sprintf("%d%%", (0:4)*25)) +
    scale_y_sqrt("Density") + scale_fill_manual(values = c("red","blue")) + ggtitle(label = "Label Separation Density Curve")
  
  # Draw ROC curve
  plot(roc.perf, main="ROC with Convex Hull", colorize=TRUE,print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2)
  ch = performance(pred.rocr , "rch")
  plot(ch, add = TRUE, lty = 2)
  
  # Recall-Precision Plot
  plot(recall.perf,colorize = T,print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2,main = "Recall-Precision Plot")
  
  # Sensitivity-Specificity Plot
  plot(sensspec.perf,colorize = T,print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2,main = "Sensitivity vs Specificity")
  
  # Lift Plot
  plot(lift.perf,colorize = T,print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2,main = "Lift Plot")
  
  # Accuracy - Boxplot (Spread)
  plot(accuracy.perf, avg= "vertical", spread.estimate="boxplot", show.spread.at= seq(0.1, 1.0, by=0.1),main = "Accuracy - Boxplot (Spread)")
  
  # Accuracy vs Cutoff
  # Get the cutoff for the best accuracy
  bestAccInd <- which.max(accuracy.perf@"y.values"[[1]])
  bestMsg <- paste("best accuracy=", accuracy.perf@"y.values"[[1]][bestAccInd],"at cutoff=", round(accuracy.perf@"x.values"[[1]][bestAccInd], 4))
  plot(accuracy.perf, sub=bestMsg,main = "Accuracy vs Cutoff")
  
  # TPR vs Cutoff
  plot(tpr.perf,main = "TPR vs Cutoff")
  
  # TNR vs Cutoff
  plot(tnr.perf,main = "TNR vs Cutoff")
  
  # FPR vs Cutoff
  plot(fpr.perf,main = "FPR vs Cutoff")
  
  # FNR vs Cutoff
  plot(fnr.perf,main = "FNR vs Cutoff")
  
  # Prediction-conditioned miss 
  plot(pcmiss.perf, colorize=T, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(1.2,1.2), avg="threshold", lwd=3)
  
  ## Various Metric Plots ##
  # pos/neg densities
  print(ggplot(data=pred.df,aes(x=predicted)) + geom_density(aes(fill=factor(actual)), size=1, alpha=.3) +
          scale_x_continuous("Predicted", breaks=(0:4)/4, limits=c(0,1), labels=sprintf("%d%%", (0:4)*25)) +
          scale_y_sqrt("Density") + scale_fill_manual(values = c("red","green")) + ggtitle(label = "Label Separation Density Curve"))
  
  
  # Confusion Matrix
  confusionMatrix <- table(predDT ,testdata[,pY])
  confusionMatrix
  
  ore.save(confusionMatrix,name = ds.name,append = TRUE)
  
  # Basic measures calculation from CM
  TP <- confusionMatrix[1,1]
  TN <- confusionMatrix[2,2]
  FN <- confusionMatrix[1,2]
  FP <- confusionMatrix[2,1]
  TOT <- sum(confusionMatrix)      
  
  # Measures
  Accuracy <- round(as.numeric((TP + TN)/TOT),3)
  ErrorRate <- 1 - Accuracy
  FPR <- round(as.numeric(FP/(TN+FP)),3)
  Recall <- round(as.numeric(TP/(TP+FN)),3)
  Specificity <- round(as.numeric(TN/(TN + FP)),3)
  Precision <- round(as.numeric(TP/(TP+FP)),3)
  Fvalue <- round(as.numeric(2*Recall*Precision/(Recall + Precision)),3)
  
  df <- rbind(auc_score = auc_score,prbe_score = prbe.score,acc_rocr = acc_rate,accuracy = Accuracy,acc_rocr_gt50 = acc_roc_gt_50,error_rate_rocr = error_rate,error_rate = ErrorRate,fpr = FPR,recall = Recall,specificity = Specificity,precision = Precision,fval = Fvalue)
  #colnames(df) <- "scores"
  perf_metric <- data.frame(name=rownames(df),score=df,row.names = NULL)   
  perf_metric
}                

# -------------------------------------------------------------------------
## 4.Model Decision Tree prediction
# -------------------------------------------------------------------------
mdl.pred.RF <- function(model,validationset,predtype="class")
{
  # predicting on the validation Set
  predtree <- predict(object = model,newdata = validationset,type = predtype)  
  return(predtree)
}    

# -------------------------------------------------------------------
## 5.Data Imputation
#--------------------------------------------------------------------

impute.data <- function(data,imputetype="mean")
{
  ## This function imputes NA by mean or median values
  if(imputetype == "mean"){
    for (i in which(sapply(data, is.numeric))) {
      data[is.na(data[, i]), i] <- mean(data[, i],  na.rm = TRUE)
    }
  } else if(imputetype == "median") {
    for (i in which(sapply(data, is.numeric))) {
      data[is.na(data[, i]), i] <- median(data[, i],  na.rm = TRUE)
    }
  } else if(imputetype == "knn"){
    library(DMwR)      
    data <- knnImputation(data = data)
  }else{
    stop("wrong imputation type.Only mean,median and knn is supported")
  }
  
  return(data)
}	

# -------------------------------------------------------------------
## 6.Converting formula to vectors of string
#--------------------------------------------------------------------	

x_variables <- function(form,Y)
{
  aa <- gsub(pattern =" ",replacement="",x=paste0(format(form), collapse = ""))
  bb <- gsub(pattern = "[+~]",replacement=",",x=aa)
  cc <- unlist(strsplit(x = bb,split = "[,]"))
  xcols <- cc[!cc %in% c(Y)]			  
  return(xcols)
}

imbalance_correction <- function(form,data,perc.over,perc.under,k,isRequired)
{
  # Handling class Imbalance
  library(DMwR)
  
  if(isRequired == 'Y'){
    data_bal <- SMOTE(form = form,data = data,perc.over = perc.over,k = k,perc.under = perc.under)
    return(data_bal)
  }
  else{
    return(data)
  }
}

######################################################
## THE MAIN FUNCTION ##
## This is the Entry point
######################################################                  	    

# Pulling dataset into ORE transparency Layer
dataset <- ore.pull(p_dataframe)                    

# Releveling
dataset[ ,pY] <- relevel(factor(dataset[ ,pY]),prim_class_label)            

## Data splitting into training and validation set
splt <- data.split(df = dataset,resVal = pY,seedvalue = 1000,spltratio = p_spltratio)

# Creation of training and validation set
training <- subset(x = dataset,splt == TRUE)
validation <- subset(x = dataset,splt == FALSE)

# Handling class Imbalance
training_bal <- imbalance_correction(form = p_formula,data = training[colnames(training)!='ORIG_HIRE_DT'],perc.over = perc_over,perc.under = perc_under,k = kn,isRequired = isR)

# Imputing the NA on main dataset
training_imputed <- impute.data(data = training_bal,imputetype = p_imputetype)
validation_imputed <- impute.data(data = validation,imputetype = p_imputetype)    

print("#################################################")
print("***1.Model Building -  RandomForest...")
print("#################################################")

# Extracting only X columns
p_trainx <- training_imputed[ ,x_variables(p_formula,pY)]

# Extracting Y column
p_trainy <- training_imputed[,pY]

## Conversion of CHAR columns to FACTOR
# Training Data Predictors
p_trainx[sapply(p_trainx, is.character)] <- lapply(p_trainx[sapply(p_trainx, is.character)], as.factor)

# Validation Data Predictors
validation_imputed[sapply(validation_imputed, is.character)] <- lapply(validation_imputed[sapply(validation_imputed, is.character)], as.factor)

print("***2.Training the RF Model")
modRF <- mdl.training.RF(p_trainx = p_trainx,p_trainy = p_trainy,p_ntree = pntree)

print("***3.Random Forest Variable Importance")
mdlRF_Imp <- mdl.rf.var.importance(modelfit = modRF,title = "RF variable Importance",topn = p_topn,barcolor = "brown")
mdlRF_Imp

print("#################################################")
print("***.COMPLETED - Model Building -  Random Forest...")
print("#################################################")

#print("Saving the RF Model")
#save(modRF,file = paste0(p_storageDir,"/","trainedModRF.rda"))

if (nrow(ore.datastore(name=ds.name)) > 0 ) 
{
  ore.delete(name = ds.name)
}
ore.save(modRF,name = ds.name,append = TRUE)  
ore.save(training,name = ds.name,append = TRUE)  

print("***3.Model Metrics...")

summary(validation_imputed)
mdl_metric_RF <- metricROCR(model = modRF,testdata = validation_imputed,pY = pY,ds.name = ds.name,p_class_label=prim_class_label)

# Returning the Metrics                   
mdl_metric_RF
